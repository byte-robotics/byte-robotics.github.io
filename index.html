<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<!-- saved from url=(0022)http://shijianping.me/ -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" slick-uniqueid="3"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<meta name="keywords" content="Tao Kong, Kong Tao, Tsinghua, object detection, deep learning, computer vision"> 
<meta name="description" content="Tao Kong&#39;s home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<style type="text/css">
</style>
<title>Tao Kong</title>
<script type="text/javascript" async="" src="ga.js"></script><script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-39824124-1']);
  _gaq.push(['_trackPageview']);

    function show_switch(obj_name) {
        var obj = document.getElementById(obj_name);
        
        if (obj.style.display == "none") {
            obj.style.display = "block";
        }
        else {
            obj.style.display = "none";
        }
    }

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);k
  })();

</script>
</head>
<body>

<div id="layout-content" style="margin-top:25px">

<table>
    <tbody>
        <tr>
			<td width = "350">
				<img src="me.jpg" border="20" width="280">
			</td>
            <td width="670">
                <div id="toptitle">                 
                    <h1>Tao Kong (孔涛)</h1><h1>
                </h1></div>

                <p>Research Scientist@Bytedance AI Lab<br>
		   Computer Vision, Machine Learning and Robotics<br>
                    <br>
                    Email: <a href="mailto:taokongcn@gmail.com">taokongcn@gmail.com</a><br>
			   [<a href="https://scholar.google.com/citations?user=kSUXLPkAAAAJ&hl=en">Google Scholar</a>], [<a href="https://github.com/taokong/">Github</a>], [<a href="https://www.zhihu.com/people/kong-tao-72">ZHIHU</a>]
                </p>
            </td>

            <td>&nbsp;</td>
        </tr><tr>
    </tr></tbody>
</table>

  
<h2>Biography</h2>
<p>
    Tao Kong is currently a research scientist at <a href="https://ailab.bytedance.com/">Bytedance AI Lab</a>. 
    Before that, he received Ph.D. degree of computer science in <a href="http://www.tsinghua.edu.cn">Tsinghua University</a> with honors, under the supervision of <a href="https://scholar.google.com/citations?user=DbviELoAAAAJ&hl=en">Prof. Fuchun Sun</a>.
</p>
<p>From Oct 2018 to Mar 2019, he visitted Grasp Laboratory in University of Pennsylvania, supervised by <a href="http://www.cis.upenn.edu/~jshi">Prof. Jianbo Shi</a>. Previously, he co-founded a promising start-up company, which aimed at machine vision based industry applications and products. He also interned at Visual Computing Group of MSRA （with <a href="https://jifengdai.org">Dr. Jifeng Dai</a> and <a href="https://sites.google.com/site/hanhushomepage">Dr. Han Hu</a>） and Cognitive Computing Lab of Intel (with <a href="https://yaoanbang.github.io">Dr. Anbang Yao</a>).</p>
 
<p> </p>
<p style="color:red">We are recruiting self-motivated interns / full-time researchers and developers in vision / language understanding, and especially, cross-modal machine learning. If you are interested, please drop me an email.</p>
<p> </p>

<h2>Selected Publications [<a href="https://scholar.google.com/citations?user=kSUXLPkAAAAJ&hl=en">full</a>]</h2>
<p>(* Interns, <sup>+</sup> Equal contribution)</p>
<ul> 
	<li>
      <a href="https://arxiv.org/abs/1912.04488">SOLO: Segmenting Objects by Locations</a><br>
	     Xinlong Wang*, <b>Tao Kong</b>, Chunhua Shen, Yuning Jiang and Lei Li<br>
                Tech report, arXiv, 2019<br>
		[<a href="https://arxiv.org/abs/1912.04488">arXiv</a>][<a href="http://www.taokong.org/">code</a>]<br>
        <p></p>
      </li>
	      
      <li>
      <a href="https://arxiv.org/abs/1909.07701">Task-Aware Monocular Depth Estimation for 3D Object Detection
</a><br>
	     Xinlong Wang*, Wei Yin, <b>Tao Kong</b>, Yuning Jiang, Lei Li and Chunhua Shen<br>
                AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2020 (<font color="red">Oral</font>)<br>
		[<a href="https://arxiv.org/abs/1909.07701">arXiv</a>][<a href="https://github.com/WXinlong/ForeSeE">code</a>]<br>
        <p></p>
      </li>
      <li>
      <a href="https://arxiv.org/abs/1904.03797">FoveaBox: Beyond Anchor-based Object Detector</a><br>
	     <b>Tao Kong</b>, Fuchun Sun, Huaping Liu, Yuning Jiang, Jianbo Shi<br>
                Tech report, arXiv, 2019<br>
		[<a href="https://arxiv.org/abs/1904.03797">arXiv</a>][<a href="https://github.com/taokong/FoveaBox">code</a>][<a href="https://github.com/open-mmlab/mmdetection/tree/master/configs/foveabox">code@mmdet</a>]<br>
        <p></p>
    </li>
	
      <li>
      <a href="https://ieeexplore.ieee.org/document/8721661/authors#authors">Feature Pyramid Reconfiguration with Consistent Loss for Object Detection
</a>,<br>
	     <b>Tao Kong</b><sup>+</sup>, Fuchun Sun<sup>+</sup>,  Wenbing Huang, Chuanqi Tan, Bin Fang, Huaping Liu<br>
                <em>IEEE Transactions on Image Processing</em> (<b>TIP</b>), 2019<br>
        <p></p>
    </li>
	<li>
      <a href="https://arxiv.org/abs/1812.01210">Zoom-In-to-Check: Boosting Video Interpolation via Instance-level Discrimination
</a><br>
	     Liangzhe Yuan<sup>+</sup>, Yibo Chen<sup>+</sup>, Hantian Liu, <b>Tao Kong</b>, Jianbo Shi<br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2019<br>
		[<a href="https://arxiv.org/abs/1812.01210">arXiv</a>][<a href="https://youtu.be/q-_wIRq26DY">demo</a>]<br>
        <p></p>
    </li>
	

     <li>
      <a href="https://arxiv.org/abs/1808.07993">Deep Feature Pyramid Reconfiguration for Object Detection</a><br>
        <b>Tao Kong</b>, Fuchun Sun, Chuanqi Tan, Huaping Liu, Wenbing Huang<br>
                <em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2018<br>
		[<a href="https://arxiv.org/abs/1808.07993">arXiv</a>][<a href="https://taokong.github.io/papers/eccv2018_kong_poster.pdf">poster</a>]<br>
        <p></p>
    </li>

     <li>
      <a href="https://ieeexplore.ieee.org/abstract/document/7989191">A hybrid deep architecture for robotic grasp detection</a><br>
        Di Guo, Fuchun Sun, Huaping Liu, <b>Tao Kong</b>, Bin Fang, Ning Xi<br>
                <em>IEEE International Conference on Robotics and Automation</em> (<b>ICRA</b>), 2017 (<font color="red">Oral</font>)<br>
        <p></p>
    </li>

     <li>
      <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Kong_RON_Reverse_Connection_CVPR_2017_paper.pdf">RON: Reverse Connection with Objectness Prior Networks for Object Detection</a><br>
        <b>Tao Kong</b>, Fuchun Sun, Anbang Yao, Huaping Liu, Yurong Chen, Ming Lu<br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2017<br>
	     	[<a href="https://arxiv.org/abs/1707.01691">arXiv</a>][<a href="https://github.com/taokong/RON">code</a>][<a href="https://taokong.github.io/papers/RON_introduction.pdf">slides</a>][<a href="https://taokong.github.io/papers/ron_cvpr17_poster.pdf">poster</a>][<a href="https://youtu.be/VKZMmHx36SE">demo</a>]<br>
        <p></p>
    </li>
	
    <li>
      <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Kong_HyperNet_Towards_Accurate_CVPR_2016_paper.pdf">HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection</a><br>
        <b>Tao Kong</b>, Anbang Yao, Yurong Chen, Fuchun Sun<br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2016 (<font color="red">Spotlight</font>) <br>
        <p></p>
    </li>

    <li>
      <a href="http://ieeexplore.ieee.org/document/7487351/?tp=&arnumber=7487351&url=http:%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7487351">
      Object discovery and grasp detection with a shared convolutional neural network<br></a>
        Di Guo, <b>Tao Kong</b>, Fuchun Sun, Huaping Liu<br>
            <em> IEEE International Conference on Robotics and Automation</em> (<b>ICRA</b>), 2016 (<font color="red">Oral</font>)  <br>
        <p></p>
    </li>
    2015 and before<br>
    <li>
      <a href="http://ieeexplore.ieee.org/document/7487351/?tp=&arnumber=7487351&url=http:%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7487351">
      Object discovery and grasp detection with a shared convolutional neural network<br></a>
        Di Guo, <b>Tao Kong</b>, Fuchun Sun, Huaping Liu<br>
            <em> IEEE International Conference on Robotics and Automation</em> (<b>ICRA</b>), 2016 (<font color="red">Oral</font>)  <br>
        <p></p>
   </li>
</ul> 

	
	
<h2>Academic Service</h2>
<ul>    
    <li>
    Conference Reviewer: CVPR, ICCV, ECCV, etc.
    </li>
    <li>
    Journal Reviewer: TPAMI, TIP, etc.
    </li>
</ul>


<h2>Media Coverage</h2>
<ul>  
    <li>
     <a href="https://mp.weixin.qq.com/s/WAjIWSLgJLVE0KsJZdtqGA">字节跳动实习生提出实例分割新方法-SOLO</a>, QbitAI News, 2019-12
    </li>
    <li>
     <a href="https://www.jiqizhixin.com/articles/2019-05-05-10">最新Anchor-Free目标检测模型—FoveaBox</a>, Synced News, 2019-05
    </li>
    <li>
    <a href="http://www.sohu.com/a/156480214_473283">CVPR清华大学研究，高效视觉目标检测框架-RON</a>, Sohu News, 2017-07
    </li>
    <li>
    <a href="http://news.tsinghua.edu.cn/publish/thunews/10303/2016/20161026134903594198295/20161026134903594198295_.html">清华大学计算机系团队获得国际机器人抓取与操作比赛冠军</a>, Tsinghua News, 2016-10
    </li>
</ul>   

<p>　</p>
<p>　</p>
<a href="http://www.easycounter.com/">
<img src="//www.easycounter.com/counter.php?taokongcn"
border="0" alt="Free Hit Counters"></a>
<br><a href="http://www.easycounter.com/">visitors
        since May 2016</a>

<p align="left"><i>Last update: Nov, 2019</i> </p>


</body></html>
